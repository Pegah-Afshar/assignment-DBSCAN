{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_user = 'dsbc_student'\n",
    "postgres_pw = '7*.8G9QH21'\n",
    "postgres_host = '142.93.121.174'\n",
    "postgres_port = '5432'\n",
    "postgres_db = 'heartdisease'\n",
    "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(\n",
    "postgres_user, postgres_pw, postgres_host, postgres_port, postgres_db))\n",
    "\n",
    "df = pd.read_sql_query('select * from heartdisease',con=engine)\n",
    "\n",
    "# No need for an open connection, \n",
    "# because you're only doing a single query\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Apply DBSCAN to the heart disease dataset by trying different values for the eps and min_samples parameters. You'll realize that it's really hard—if not impossible—to get a two-cluster solution using DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and the outcome\n",
    "X = df.iloc[:, :13]\n",
    "y = df.iloc[:, 13]\n",
    "\n",
    "# Replace missing values (marked by `?`) with a `0`\n",
    "X = X.replace(to_replace='?', value=0)\n",
    "\n",
    "# Binarize y so that `1` means heart disease diagnosis and `0` means no diagnosis\n",
    "y = np.where(y > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "X_std=scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan=DBSCAN(eps=1,min_samples=5)\n",
    "cluster=dbscan.fit_predict(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index of the Agglomerative Clustering solution: 0.21394030618551016\n",
      "The silhouette score of the Agglomerative Clustering solution: 0.11730765444448985\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agg_cluster=AgglomerativeClustering(linkage='complete',affinity='cosine',n_clusters=2)\n",
    "cluster=agg_cluster.fit_predict(X_std)\n",
    "print(\"Adjusted Rand Index of the Agglomerative Clustering solution: {}\"\n",
    "      .format(metrics.adjusted_rand_score(y, cluster)))\n",
    "print(\"The silhouette score of the Agglomerative Clustering solution: {}\"\n",
    "      .format(metrics.silhouette_score(X_std, cluster, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index of the Agglomerative Clustering solution: 0.2940490133353465\n",
      "The silhouette score of the Agglomerative Clustering solution: 0.14837359969689895\n"
     ]
    }
   ],
   "source": [
    "agg_cluster=AgglomerativeClustering(linkage='average',affinity='cosine',n_clusters=2)\n",
    "cluster=agg_cluster.fit_predict(X_std)\n",
    "print(\"Adjusted Rand Index of the Agglomerative Clustering solution: {}\"\n",
    "      .format(metrics.adjusted_rand_score(y, cluster)))\n",
    "print(\"The silhouette score of the Agglomerative Clustering solution: {}\"\n",
    "      .format(metrics.silhouette_score(X_std, cluster, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index of the Agglomerative Clustering solution: 0.14859596899025246\n",
      "The silhouette score of the Agglomerative Clustering solution: 0.155240382445262\n"
     ]
    }
   ],
   "source": [
    "agg_cluster=AgglomerativeClustering(linkage='ward',affinity='euclidean',n_clusters=3)\n",
    "cluster=agg_cluster.fit_predict(X_std)\n",
    "print(\"Adjusted Rand Index of the Agglomerative Clustering solution: {}\"\n",
    "      .format(metrics.adjusted_rand_score(y, cluster)))\n",
    "print(\"The silhouette score of the Agglomerative Clustering solution: {}\"\n",
    "      .format(metrics.silhouette_score(X_std, cluster, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Apply agglomerative clustering to the heart disease data by setting n_clusters=2. Try the three linkage methods above, and get ARI and silhouette scores for each of your solutions. Compare the results with each other. Then compare the results with the results from the k-means solution that you implemented in the previous checkpoint's assignment. Which algorithm and setting perform better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index of the Agglomerative Clustering solution: 0.21394030618551016\n",
      "The silhouette score of the Agglomerative Clustering solution: 0.11730765444448985\n"
     ]
    }
   ],
   "source": [
    "agg_cluster=AgglomerativeClustering(linkage='complete',affinity='cosine',n_clusters=2)\n",
    "cluster=agg_cluster.fit_predict(X_std)\n",
    "print(\"Adjusted Rand Index of the Agglomerative Clustering solution: {}\"\n",
    "      .format(metrics.adjusted_rand_score(y, cluster)))\n",
    "print(\"The silhouette score of the Agglomerative Clustering solution: {}\"\n",
    "      .format(metrics.silhouette_score(X_std, cluster, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index of the Agglomerative Clustering solution: 0.2940490133353465\n",
      "The silhouette score of the Agglomerative Clustering solution: 0.14837359969689895\n"
     ]
    }
   ],
   "source": [
    "agg_cluster=AgglomerativeClustering(linkage='average',affinity='cosine',n_clusters=2)\n",
    "cluster=agg_cluster.fit_predict(X_std)\n",
    "print(\"Adjusted Rand Index of the Agglomerative Clustering solution: {}\"\n",
    "      .format(metrics.adjusted_rand_score(y, cluster)))\n",
    "print(\"The silhouette score of the Agglomerative Clustering solution: {}\"\n",
    "      .format(metrics.silhouette_score(X_std, cluster, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index of the Agglomerative Clustering solution: 0.146129913123814\n",
      "The silhouette score of the Agglomerative Clustering solution: 0.1387197366557222\n"
     ]
    }
   ],
   "source": [
    "agg_cluster=AgglomerativeClustering(linkage='ward',affinity='euclidean',n_clusters=2)\n",
    "cluster=agg_cluster.fit_predict(X_std)\n",
    "print(\"Adjusted Rand Index of the Agglomerative Clustering solution: {}\"\n",
    "      .format(metrics.adjusted_rand_score(y, cluster)))\n",
    "print(\"The silhouette score of the Agglomerative Clustering solution: {}\"\n",
    "      .format(metrics.silhouette_score(X_std, cluster, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on ARI and silhouette score, average linkage method is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index of the Agglomerative Clustering solution: 0.4380857727169879\n",
      "The silhouette score of the Agglomerative Clustering solution: 0.17530682286260937\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kmeans=KMeans(n_clusters=2)\n",
    "kmeans.fit(X_std)\n",
    "y_pred = kmeans.predict(X_std)\n",
    "print(\"Adjusted Rand Index of the Agglomerative Clustering solution: {}\"\n",
    "      .format(metrics.adjusted_rand_score(y, y_pred)))\n",
    "print(\"The silhouette score of the Agglomerative Clustering solution: {}\"\n",
    "      .format(metrics.silhouette_score(X_std, y_pred, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans is the best solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
